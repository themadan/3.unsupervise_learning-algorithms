{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRn9v3jbrO9o",
        "colab_type": "text"
      },
      "source": [
        "# 1.Normalize the data\n",
        "* First step is to normalize the data that we have so that PCA works properly. This is done by subtracting the respective means from the numbers in the respective column. So if we have two dimensions X and Y, all X become ùîÅ- and all Y become ùíö-. This produces a dataset whose mean is zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzPlm2Gtq-aN",
        "colab_type": "text"
      },
      "source": [
        "# 2.Calculate the covariance matrix.\n",
        "jati dimention ko data vayo tai dimention ko covariance matrix hunxa.\n",
        "* (we can also use correlation matrix or even Single value decomposition, however in this post will focus on covariance matrix)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To6Ei-dYqlSF",
        "colab_type": "text"
      },
      "source": [
        "#  3.Calculate the eigenvalues and eigenvector\n",
        "# note:dimension of dataset(number of feature)=number of eigen value=eigen vector combination\n",
        "* Next step is to calculate the eigenvalues and eigenvectors for the covariance matrix. \n",
        "\n",
        "( ∆õI - A )v = 0\n",
        "\n",
        "* Sort eigenvalues in descending order and choose the top k Eigenvectors that correspond to the k largest eigenvalues (k will become the number of dimensions of the new feature subspace k‚â§d, d is the number of original dimensions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flt7mDEKpyaz",
        "colab_type": "text"
      },
      "source": [
        "# 4.Choosing Components and forming a feature vector:\n",
        "* It turns out that the eigenvector corresponding to the highest eigenvalue is the principal component of the dataset and it is our call as to how many eigenvalues we choose to proceed our analysis with.\n",
        "* To reduce the dimensions, we choose the first p eigenvalues and ignore the rest. We do lose out some information in the process, but if the eigenvalues are small, we do not lose much.\n",
        "* Next we form a feature vector which is a matrix of vectors, in our case, the eigenvectors. In fact, only those eigenvectors which we want to proceed with. Since we just have 2 dimensions in the running example, we can either choose the one corresponding to the greater eigenvalue or simply take both.\n",
        "\n",
        " Feature Vector = (eig1, eig2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tokmO8uvrZpm",
        "colab_type": "text"
      },
      "source": [
        "# 5.Forming Principal Components \n",
        "* NewData = FeatureVectorT x ScaledDataT\n",
        "* FeatureVector is the matrix we formed using the eigenvectors we chose to keep, and\n",
        "\n",
        "ScaledData is the scaled version of original dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkk_Ot1qsPjZ",
        "colab_type": "text"
      },
      "source": [
        "# NOTE:\n",
        "* All the eigenvectors of a matrix are perpendicular to each other. So, in PCA, what we do is represent or transform the original dataset using these orthogonal (perpendicular) eigenvectors instead of representing on normal x and y axes.\n",
        "# GOAL OF PCA:\n",
        "* The typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes.\n",
        "* However, the eigenvectors only define the directions of the new axis, since they have all the same unit length 1.\n",
        "# How to select the new set of Principal components:\n",
        "* The rule behind is that we sort the Eigenvalues in descending order and then choose the top k features concerning top k Eigenvalues.\n",
        "* The idea here is that by choosing top k we have decided that the variance which corresponds to those k feature space is enough to describe the data set. And by losing the remaining variance of those not selected features, won‚Äôt cost the accuracy much or we are OK to loose that much accuracy that costs because of neglected variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLpZx8B_i5pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}